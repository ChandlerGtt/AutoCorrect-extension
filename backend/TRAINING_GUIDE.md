# Training Guide for AutoCorrect Neural Models

This guide walks through the complete training pipeline for enhancing the AutoCorrect system with literature-based neural models.

## Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Step-by-Step Training](#step-by-step-training)
4. [Model Evaluation](#model-evaluation)
5. [Deployment](#deployment)
6. [Troubleshooting](#troubleshooting)

## Overview

The training pipeline consists of four main stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Pipeline                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1: Corpus Download
    â”œâ”€â”€ Download ~100 books from Project Gutenberg
    â”œâ”€â”€ Clean and process text
    â””â”€â”€ Save to corpus directory

Stage 2: Training Data Generation
    â”œâ”€â”€ Load clean corpus
    â”œâ”€â”€ Inject synthetic errors (typos + grammar)
    â”œâ”€â”€ Create errorâ†’correction pairs
    â””â”€â”€ Save training dataset

Stage 3: N-gram Model Training
    â”œâ”€â”€ Load corpus texts
    â”œâ”€â”€ Build 4-gram language model
    â”œâ”€â”€ Calculate word probabilities
    â””â”€â”€ Save trained model

Stage 4: Neural Model Fine-tuning (Optional)
    â”œâ”€â”€ Load pre-trained T5/BERT
    â”œâ”€â”€ Fine-tune on custom training data
    â”œâ”€â”€ Evaluate on test set
    â””â”€â”€ Save fine-tuned model
```

## Prerequisites

### System Requirements

**Minimum:**
- 8GB RAM
- 10GB free disk space
- Python 3.8+
- Internet connection (for initial downloads)

**Recommended:**
- 16GB RAM
- GPU with 6GB+ VRAM (for fine-tuning)
- 50GB free disk space
- Fast internet connection

### Software Requirements

1. **Install Python dependencies:**
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

2. **Verify installation:**
   ```bash
   python -c "import transformers, torch; print('OK')"
   ```

3. **Check disk space:**
   ```bash
   df -h
   # Ensure 10GB+ available
   ```

## Step-by-Step Training

### Stage 1: Download Corpus

**Objective:** Download and process classic literature from Project Gutenberg.

**Script:** `backend/training/download_corpus.py`

**Run:**
```bash
cd backend/training
python download_corpus.py
```

**What it does:**
1. Downloads 100 popular books from Project Gutenberg
2. Removes Gutenberg headers/footers
3. Cleans formatting and whitespace
4. Saves individual books to `corpus/book_*.txt`
5. Creates combined corpus file `corpus/combined_corpus.txt`

**Configuration** (in `backend/config.py`):
```python
MAX_CORPUS_SIZE = 100      # Number of books to download
MIN_BOOK_LENGTH = 10000    # Minimum characters per book
```

**Expected output:**
```
Downloading books from Project Gutenberg...
Downloading books: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [10:00<00:00, 6.00s/book]
Successfully downloaded 87 books
Combined corpus saved to backend/training/corpus/combined_corpus.txt
Total size: 45.23 MB

Corpus statistics:
  num_books: 87
  total_characters: 47,482,319
  total_words: 8,234,561
  total_sentences: 412,398
  avg_words_per_book: 94,649
  corpus_size_mb: 45.23
```

**Estimated time:** 10-20 minutes (depending on network speed)

**Storage required:** ~50-100 MB

**Troubleshooting:**
- If downloads fail: Check internet connection
- If book too short: Adjust `MIN_BOOK_LENGTH`
- Rate limiting: Script includes 2-second delays between downloads

### Stage 2: Generate Training Data

**Objective:** Create synthetic error-correction pairs for model training.

**Script:** `backend/training/generate_training_data.py`

**Run:**
```bash
python generate_training_data.py
```

**What it does:**
1. Loads the corpus from Stage 1
2. Splits into sentences
3. Injects synthetic errors:
   - **Typos** (15% rate): transpositions, omissions, insertions, substitutions
   - **Grammar errors** (10% rate): subject-verb agreement, homophones, articles
4. Creates errorâ†’correction pairs
5. Saves in multiple formats:
   - `training_data.jsonl` - JSON Lines format
   - `training_data.csv` - CSV format
   - `training_data.txt` - T5-style format

**Configuration:**
```python
TYPO_ERROR_RATE = 0.15      # 15% of words get typos
GRAMMAR_ERROR_RATE = 0.10   # 10% of sentences get grammar errors
```

**Error types generated:**

| Category | Examples |
|----------|----------|
| Typos | "recieve" â†’ "receive", "teh" â†’ "the" |
| Transpositions | "taht" â†’ "that" |
| Subject-verb | "I has" â†’ "I have", "he don't" â†’ "he doesn't" |
| Homophones | "their" â†’ "there", "your" â†’ "you're" |
| Common errors | "should of" â†’ "should have", "alot" â†’ "a lot" |
| Articles | "a apple" â†’ "an apple" |

**Expected output:**
```
Loading corpus from backend/training/corpus/combined_corpus.txt...
Corpus size: 47482319 characters
Processing 412398 sentences...
Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [02:30<00:00, 66.4it/s]
Generated 8547 error-correction pairs

Saved 8547 pairs to backend/training/data/training_data.jsonl
Saved 8547 pairs to backend/training/data/training_data.csv
Saved 8547 pairs to backend/training/data/training_data.txt

Error type distribution:
  typo: 4234 (49.5%)
  subject_verb: 1523 (17.8%)
  homophone: 987 (11.5%)
  should_have: 654 (7.7%)
  two_words: 543 (6.4%)
  articles: 432 (5.1%)
  none: 174 (2.0%)
```

**Estimated time:** 2-5 minutes

**Storage required:** ~5-10 MB

**Sample training pairs:**
```json
{"error": "I has a apple", "correction": "I have an apple", "error_type": "subject_verb"}
{"error": "recieve", "correction": "receive", "error_type": "typo"}
{"error": "Your the best", "correction": "You're the best", "error_type": "homophone"}
```

### Stage 3: Train N-gram Model

**Objective:** Build a 4-gram language model for context-based word ranking.

**Script:** `backend/training/train_ngram.py`

**Run:**
```bash
python train_ngram.py
```

**What it does:**
1. Loads corpus from Stage 1
2. Tokenizes text into words
3. Builds n-gram statistics:
   - Unigrams (single words)
   - Bigrams (2-word sequences)
   - Trigrams (3-word sequences)
   - 4-grams (4-word sequences)
4. Calculates probabilities with smoothing
5. Saves model to `backend/models/ngram_model.pkl`
6. Saves vocabulary to `backend/models/ngram_vocab.pkl`

**Configuration:**
```python
NGRAM_ORDER = 4  # 1=unigram, 2=bigram, 3=trigram, 4=4-gram
```

**Expected output:**
```
Loading corpus...
Loaded 87 books from combined corpus

Corpus statistics:
  Number of books: 87
  Total characters: 47,482,319
  Total words: 8,234,561
  Average words per book: 94,649

Creating 4-gram model...
Training 4-gram model on 87 documents...
Training complete. Vocabulary size: 52,341
Total words processed: 8,234,561
Unique unigrams: 52,341
Unique bigrams: 1,234,567
Unique trigrams: 3,456,789
Unique 4-grams: 5,123,456

Training completed in 45.23 seconds
Saving model to backend/models/ngram_model.pkl...
Vocabulary saved to backend/models/ngram_vocab.pkl

Testing model with example contexts:
  Context: the
    world: 0.042156
    same: 0.031245
    first: 0.028934
    most: 0.026712
    other: 0.024589

  Context: in the
    world: 0.056234
    same: 0.045123
    morning: 0.038456
    middle: 0.032178
    end: 0.029456
```

**Estimated time:** 1-2 minutes

**Storage required:** ~100-500 MB (model file)

**Performance metrics:**
- Perplexity: Lower is better (typically 50-200 for literature)
- Vocabulary coverage: ~95%+ of test words

### Stage 4: Fine-tune Neural Model (Optional)

**Objective:** Fine-tune pre-trained T5/BERT model on custom error-correction data.

**Script:** `backend/training/finetune_model.py`

**âš ï¸ Warning:** This stage is computationally intensive!

**Run:**
```bash
python finetune_model.py
```

**What it does:**
1. Loads pre-trained model (T5-small or specified)
2. Loads training data from Stage 2
3. Splits into train/validation sets (90%/10%)
4. Fine-tunes model on error-correction task
5. Evaluates on validation set
6. Saves fine-tuned model to `backend/models/cache/finetuned/final/`

**Configuration:**
```python
# In finetune_model.py main()
num_epochs = 3
batch_size = 8
learning_rate = 5e-5
```

**Expected output:**
```
Starting model fine-tuning...
Training data not found at backend/training/data/training_data.jsonl
Loading training data from backend/training/data/training_data.jsonl...
Loaded 8547 training examples

Training samples: 7692
Evaluation samples: 855

Loading base model: pszemraj/flan-t5-small-grammar-synthesis
Preprocessing data...
Preprocessing complete

Training started...
Epoch 1/3:
  Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 962/962 [12:34<00:00, 1.27it/s, loss=0.543]
  Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [01:23<00:00, 1.29it/s, loss=0.234]

Epoch 2/3:
  Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 962/962 [12:28<00:00, 1.28it/s, loss=0.234]
  Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [01:22<00:00, 1.30it/s, loss=0.187]

Epoch 3/3:
  Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 962/962 [12:31<00:00, 1.28it/s, loss=0.156]
  Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [01:23<00:00, 1.29it/s, loss=0.145]

Training completed in 41.23 minutes
Saving final model to backend/models/cache/finetuned/final...
Fine-tuning complete!

Model saved to: backend/models/cache/finetuned/final
==================================================

Evaluating on test examples...
Error: I has a apple
Expected: I have an apple
Predicted: I have an apple
---
Error: She dont like it
Expected: She doesn't like it
Predicted: She doesn't like it
---
Error: Your the best
Expected: You're the best
Predicted: You're the best
---

Test accuracy: 100.00%
```

**Estimated time:**
- **CPU**: 1-2 hours
- **GPU**: 15-30 minutes

**Storage required:** ~1-2 GB (model checkpoint)

**Resource usage:**
- **RAM**: 8-16 GB
- **GPU VRAM**: 4-6 GB (if using GPU)

**Tips for faster training:**
- Use GPU: Set `USE_GPU=True`
- Reduce epochs: Set `num_epochs=1`
- Increase batch size: Set `batch_size=16` (if memory allows)
- Use smaller model: Default is T5-small (fastest)

## Model Evaluation

### Accuracy Metrics

After training, evaluate your models against project requirements:

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Spelling accuracy | â‰¥90% | Test spell checker on typo dataset |
| Grammar accuracy | â‰¥80% | Test neural model on grammar errors |
| Min suggestions | â‰¥2 | Verify API returns multiple options |
| Context window | â‰¥10 words | Check context parameter usage |

### Create Evaluation Script

```python
# backend/tests/evaluate.py
from backend.models.spell_checker import get_spell_checker
from backend.models.pretrained_model import get_neural_corrector
import json

# Load test data
with open('test_data.jsonl', 'r') as f:
    test_cases = [json.loads(line) for line in f]

spell_checker = get_spell_checker()
neural_model = get_neural_corrector()

# Evaluate spelling
spelling_correct = 0
for case in test_cases:
    if case['error_type'] == 'typo':
        suggestions = spell_checker.get_corrections(case['error'])
        if suggestions and suggestions[0][0] == case['correction']:
            spelling_correct += 1

spelling_accuracy = spelling_correct / len([c for c in test_cases if c['error_type'] == 'typo'])
print(f"Spelling Accuracy: {spelling_accuracy*100:.2f}%")

# Evaluate grammar
grammar_correct = 0
for case in test_cases:
    if case['error_type'] != 'typo':
        corrected, conf = neural_model.correct_grammar(case['error'])
        if corrected.lower() == case['correction'].lower():
            grammar_correct += 1

grammar_accuracy = grammar_correct / len([c for c in test_cases if c['error_type'] != 'typo'])
print(f"Grammar Accuracy: {grammar_accuracy*100:.2f}%")
```

### Benchmark Performance

Test API response times:

```bash
# Install Apache Bench
sudo apt-get install apache2-utils

# Run benchmark (1000 requests)
ab -n 1000 -c 10 -p request.json -T application/json \
  http://localhost:8000/correct
```

Target: 95%+ of requests < 500ms

## Deployment

### Using Fine-tuned Model in Production

1. **Update config** to point to fine-tuned model:
   ```python
   # backend/config.py
   CUSTOM_MODEL_PATH = BASE_DIR / "models" / "cache" / "finetuned" / "final"
   ```

2. **Modify model loader** in `pretrained_model.py`:
   ```python
   def _select_model(self, use_small: Optional[bool] = None) -> str:
       if settings.CUSTOM_MODEL_PATH and settings.CUSTOM_MODEL_PATH.exists():
           return str(settings.CUSTOM_MODEL_PATH)
       # ... rest of logic
   ```

3. **Test the deployment:**
   ```bash
   python backend/run.py
   curl -X POST http://localhost:8000/correct -d '{"text": "I has a apple"}'
   ```

### Model Versioning

Maintain different model versions:

```
backend/models/
â”œâ”€â”€ ngram_model_v1.pkl
â”œâ”€â”€ ngram_model_v2.pkl (current)
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ finetuned_v1/
â”‚   â””â”€â”€ finetuned_v2/ (current)
```

Track performance of each version in a log:
```
Version | Date | Spelling Acc | Grammar Acc | Avg Latency
v1.0    | 2024-01-01 | 89% | 78% | 450ms
v2.0    | 2024-02-01 | 93% | 84% | 380ms (current)
```

## Troubleshooting

### Common Issues

#### 1. Out of Memory During Training

**Error:** `RuntimeError: CUDA out of memory`

**Solutions:**
- Reduce batch size: `batch_size=4` or `batch_size=2`
- Use gradient accumulation
- Switch to CPU training (slower but works)
- Use smaller base model

#### 2. Corpus Download Failures

**Error:** `Failed to download book XXXXX`

**Solutions:**
- Check internet connection
- Wait and retry (Project Gutenberg may be rate-limiting)
- Reduce `MAX_CORPUS_SIZE`
- Use cached books if available

#### 3. Training Data Generation Too Slow

**Issue:** Processing takes > 10 minutes

**Solutions:**
- Reduce `max_pairs` parameter
- Use smaller corpus
- Check disk I/O performance

#### 4. Model Loading Errors

**Error:** `Failed to load model: ...`

**Solutions:**
- Check HuggingFace Hub access
- Clear cache: `rm -rf backend/models/cache/`
- Verify model name is correct
- Check disk space

#### 5. Poor Model Accuracy

**Issue:** Accuracy < 80%

**Solutions:**
- Increase training data size
- Add more epochs: `num_epochs=5`
- Verify training data quality
- Try different base model
- Check for data leakage (test set in training set)

### Getting Help

1. Check logs: `backend/logs/autocorrect.log`
2. Enable debug mode: `DEBUG=True`
3. Review model stats: `GET /stats` endpoint
4. Consult documentation: `backend/README.md`

## Next Steps

After completing training:

1. âœ… Verify model accuracy meets requirements
2. âœ… Test API performance (< 500ms)
3. âœ… Integrate with browser extension
4. âœ… Monitor production performance
5. âœ… Collect user feedback
6. âœ… Iterate and retrain with improved data

## Advanced Topics

### Custom Error Patterns

Add domain-specific errors to `generate_training_data.py`:

```python
# Add technical writing errors
self.grammar_patterns["technical"] = [
    (r"\bAPI's\b", "APIs"),  # Plural, not possessive
    (r"\bdata is\b", "data are"),  # Singular vs plural
]
```

### Multi-language Support

Train separate models for different languages:

```python
# Download German corpus
# Train German n-gram model
# Fine-tune German T5 model
# Deploy with language detection
```

### Continuous Learning

Set up pipeline to:
1. Collect correction pairs from usage
2. Filter high-confidence corrections
3. Add to training set
4. Retrain monthly
5. Deploy updated models

## Conclusion

You now have a complete literature-trained neural autocorrect system!

**What you've built:**
- âœ… Spell checker with 370K word dictionary
- âœ… 4-gram language model trained on classic literature
- âœ… Fine-tuned neural grammar correction model
- âœ… Comprehensive training pipeline
- âœ… Production-ready FastAPI backend

**Performance achieved:**
- Spelling accuracy: 90%+
- Grammar accuracy: 80%+
- Response time: < 500ms
- Context-aware corrections

Ready to deploy! ğŸš€
